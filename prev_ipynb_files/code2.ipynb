{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from celluloid import Camera\n",
    "from IPython.display import HTML\n",
    "from tqdm.notebook import tqdm\n",
    "import imgaug\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
    "import imgaug.augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgg = glob.glob(\"D:\\\\Projects\\\\Brain_Tumor_ML\\\\brain_tumor_ML\\\\HGG\\\\Bra*\\\\*t1.nii*\")\n",
    "mask_hgg = glob.glob(\"D:\\\\Projects\\\\Brain_Tumor_ML\\\\brain_tumor_ML\\\\LGG\\\\Bra*\\\\*seg.nii*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\lib\\site-packages\\matplotlib\\image.py:443: UserWarning: Warning: converting a masked element to nan.\n",
      "  dv = np.float64(self.norm.vmax) - np.float64(self.norm.vmin)\n",
      "D:\\Python\\lib\\site-packages\\matplotlib\\image.py:444: UserWarning: Warning: converting a masked element to nan.\n",
      "  vmid = np.float64(self.norm.vmin) + dv / 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_path = hgg[40]\n",
    "sample_path_label = mask_hgg[40]\n",
    "\n",
    "data = nib.load(sample_path)\n",
    "label = nib.load(sample_path_label)\n",
    "\n",
    "mri = data.get_fdata()\n",
    "mask = label.get_fdata().astype(np.uint8)\n",
    "\n",
    "#  Plotting Sample Image\n",
    "fig = plt.figure()\n",
    "camera = Camera(fig)  # Create the camera object from celluloid\n",
    "\n",
    "for i in range(mri.shape[2]):  # Sagital view\n",
    "    plt.imshow(mri[:,:,i],cmap=plt.bone())\n",
    "    mask_ = np.ma.masked_where(mask[:,:,i]==0, mask[:,:,i])\n",
    "    plt.imshow(mask_, alpha=1, cmap=plt.hot())\n",
    "    camera.snap()  # Store the current slice\n",
    "\n",
    "animation = camera.animate(blit=False, interval=1) # Create the animation\n",
    "animation.save('test2.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Normalization of 3D Image\n",
    "def normalize(full_volume):\n",
    "    normal = (full_volume - full_volume.min()) / (full_volume.max() - full_volume.min())\n",
    "    return normal\n",
    "\n",
    "# For Standardization of 3D Image\n",
    "def standardize(data):\n",
    "    mu = data.mean()\n",
    "    std = np.std(data)\n",
    "    standard = (data - mu) / std\n",
    "    return standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Image with train test split\n",
    "save_root = Path(\"Preprocessed\\HGG\")\n",
    "\n",
    "for j in range(len(hgg)):\n",
    "    mri = nib.load(hgg[j])\n",
    "    # assert nib.aff2axcodes(mri.affine) == (\"R\", \"A\", \"S\")    \n",
    "    mri_data = mri.get_fdata()\n",
    "    label_data = nib.load(mask_hgg[j]).get_fdata().astype(np.uint8)\n",
    "\n",
    "    # Crop volume and label mask. Reduce 32 px from top and 32 px from bottom.\n",
    "    # Addtionally crop front and back with same size. Dont crop viewing axis\n",
    "    mri_data = mri_data[32:-32, 32:-32]\n",
    "    label_data = label_data[32:-32, 32:-32]\n",
    "\n",
    "    # Normalize and standardize the images\n",
    "    normalized_mri_data = normalize(mri_data)\n",
    "    standardized_mri_data = standardize(normalized_mri_data)\n",
    "\n",
    "    if j<60:\n",
    "        current_path = save_root/\"train\"/str(j)\n",
    "    else:\n",
    "        current_path = save_root/\"test\"/str(j)\n",
    "\n",
    "    # Loop over the slices in the full volume and store the images and labels in the data/masks directory\n",
    "    for i in range(standardized_mri_data.shape[-1]):\n",
    "        slice = standardized_mri_data[:,:,i]\n",
    "        mask = label_data[:,:,i]\n",
    "        slice_path = current_path/\"data\"\n",
    "        mask_path = current_path/\"masks\"\n",
    "        slice_path.mkdir(parents=True, exist_ok=True)\n",
    "        mask_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        np.save(slice_path/str(i), slice)\n",
    "        np.save(mask_path/str(i), mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\lib\\site-packages\\matplotlib\\image.py:443: UserWarning: Warning: converting a masked element to nan.\n",
      "  dv = np.float64(self.norm.vmax) - np.float64(self.norm.vmin)\n",
      "D:\\Python\\lib\\site-packages\\matplotlib\\image.py:444: UserWarning: Warning: converting a masked element to nan.\n",
      "  vmid = np.float64(self.norm.vmin) + dv / 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting Preprocessed Image\n",
    "\n",
    "fig = plt.figure()\n",
    "camera = Camera(fig)  # Create the camera object from celluloid\n",
    "\n",
    "path = Path(\"Preprocessed/train/1/\")  # Select a subject\n",
    "for i in range(154):\n",
    "    # Choose a file and load slice + mask\n",
    "    file = str(i+1) + \".npy\"\n",
    "    slice = np.load(path/\"data\"/file)\n",
    "    mask = np.load(path/\"masks\"/file)# Plot everything\n",
    "    plt.imshow(slice, cmap=\"bone\")\n",
    "    mask_ = np.ma.masked_where(mask==0, mask)\n",
    "    plt.imshow(mask_, cmap=\"autumn\")\n",
    "    camera.snap()\n",
    "animate = camera.animate(blit=False, interval=1)\n",
    "animate.save('prep.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dataset Class\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, augment_params):\n",
    "        self.all_files = self.extract_files(root)\n",
    "        self.augment_params = augment_params\n",
    "    \n",
    "    # Extract the paths to all slices given the root path (ends with train or val)\n",
    "    @staticmethod\n",
    "    def extract_files(root):\n",
    "        files = []\n",
    "        for subject in root.glob(\"*\"):   # Iterate over the subjects\n",
    "            slice_path = subject/\"data\"  # Get the slices for current subject\n",
    "            for slice in slice_path.glob(\"*.npy\"):\n",
    "                files.append(slice)\n",
    "        return files\n",
    "    \n",
    "    # Replace data with mask to get the masks    \n",
    "    @staticmethod\n",
    "    def change_img_to_label_path(path):\n",
    "\n",
    "        parts = list(path.parts)\n",
    "        parts[parts.index(\"data\")] = \"masks\"\n",
    "        return Path(*parts)\n",
    "\n",
    "    # Augments slice and segmentation mask in the exact same way\n",
    "    def augment(self, slice, mask):\n",
    "\n",
    "        random_seed = torch.randint(0, 1000000, (1,)).item()\n",
    "        imgaug.seed(random_seed)\n",
    "\n",
    "        mask = SegmentationMapsOnImage(mask, mask.shape)\n",
    "        slice_aug, mask_aug = self.augment_params(image=slice, segmentation_maps=mask)\n",
    "        mask_aug = mask_aug.get_arr()\n",
    "        return slice_aug, mask_aug\n",
    "    \n",
    "    # Return the length of the dataset (length of all files)\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)\n",
    "    \n",
    "    # Given an index return the (augmented) slice and corresponding mask\n",
    "    # Add another dimension for pytorch\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        file_path = self.all_files[idx]\n",
    "        mask_path = self.change_img_to_label_path(file_path)\n",
    "        slice = np.load(file_path).astype(np.float32)  # Convert to float for torch\n",
    "        mask = np.load(mask_path)\n",
    "        \n",
    "        if self.augment_params:\n",
    "            slice, mask = self.augment(slice, mask)\n",
    "\n",
    "        return np.expand_dims(slice, 0), np.expand_dims(mask, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Affine(scale=(0.85, 1.15), # Zoom in or out\n",
    "               rotate=(-45, 45)),  # Rotate up to 45 degrees\n",
    "    iaa.ElasticTransformation()  # Random Elastic Deformations\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset objects\n",
    "train_path = Path(\"Preprocessed/train/\")\n",
    "test_path = Path(\"Preprocessed/test\")\n",
    "\n",
    "train_dataset = Dataset(train_path, seq)\n",
    "test_dataset = Dataset(test_path, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
